{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb97209-3b29-48cc-83f4-17d08a2f8c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall numpy==1.26.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca4e801-811d-4f03-98c4-7606c4880e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall cvxpy==1.5.2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d171aaaf-566b-4809-bd63-7c12d4126b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas yfinance matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe63ad6-e3e4-47f2-8964-5016a4568588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "display({\"python_version\": sys.version})\n",
    "\n",
    "import numpy\n",
    "display({\"numpy_version\": numpy.__version__})\n",
    "\n",
    "import cvxpy\n",
    "display({\"cvxpy_version\": cvxpy.__version__})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfc6ca0-4445-4182-ae2c-fd946e50dd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383ede39-4ab1-447f-9060-ff4a583c7485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import cvxpy as cp\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 1. Data Collection Function\n",
    "def fetch_stock_data(tickers, start_date, end_date):\n",
    "    \"\"\"Fetch historical stock data and market caps via Yahoo Finance API\"\"\"\n",
    "    print(f\"Downloading price data for {len(tickers)} stocks...\")\n",
    "    # Get price data\n",
    "    price_data = yf.download(tickers, start=start_date, end=end_date)['Close']\n",
    "    \n",
    "    # Get latest market caps (for initial weights)\n",
    "    print(\"Fetching market cap data...\")\n",
    "    market_caps = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            # Use most recent quarter's market cap\n",
    "            market_cap = stock.info.get('marketCap', np.nan)\n",
    "            market_caps[ticker] = market_cap\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't get market cap for {ticker}: {str(e)}\")\n",
    "            market_caps[ticker] = np.nan\n",
    "    \n",
    "    return price_data, pd.Series(market_caps)\n",
    "\n",
    "# 2. CVaR Calculation\n",
    "def calculate_cvar(returns, alpha=0.95):\n",
    "    \"\"\"Calculate Conditional Value-at-Risk (CVaR)\"\"\"\n",
    "    var = np.percentile(returns, 100*(1-alpha))\n",
    "    return returns[returns <= var].mean()\n",
    "\n",
    "# 3. Portfolio Optimization\n",
    "def optimize_cvar_portfolio(returns, alpha=0.95, max_weight=0.050, min_weight=0.001):\n",
    "    \"\"\"\n",
    "    Robust optimization with solver fallback\n",
    "    Returns None if optimization fails\n",
    "    \"\"\"\n",
    "    n = returns.shape[1]\n",
    "    returns = returns.dropna()\n",
    "    \n",
    "    if len(returns) < 60:\n",
    "        print(\"Insufficient data for optimization\")\n",
    "        return None\n",
    "    \n",
    "    weights = cp.Variable(n)\n",
    "    tau = cp.Variable()\n",
    "    portfolio_returns = returns.values @ weights\n",
    "    loss = -portfolio_returns\n",
    "    cvar = tau + (1/(1-alpha)) * cp.mean(cp.pos(loss - tau))\n",
    "    \n",
    "    constraints = [\n",
    "        cp.sum(weights) <= 1 + 1e-2,\n",
    "        cp.sum(weights) >= 1 - 1e-2,\n",
    "        weights >= min_weight,\n",
    "        weights <= max_weight\n",
    "    ]\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(cvar), constraints)\n",
    "    \n",
    "    # Try Clarabel first\n",
    "    try:\n",
    "        prob.solve(solver=cp.CLARABEL)\n",
    "        if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            optimized_weights = np.maximum(weights.value, 0)\n",
    "            return optimized_weights / optimized_weights.sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Clarabel failed: {str(e)}\")\n",
    "    \n",
    "    # Fallback to ECOS\n",
    "    try:\n",
    "        prob.solve(solver=cp.ECOS)\n",
    "        if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            optimized_weights = np.maximum(weights.value, 0)\n",
    "            return optimized_weights / optimized_weights.sum()\n",
    "    except Exception as e:\n",
    "        print(f\"ECOS failed: {str(e)}\")\n",
    "    \n",
    "    return None  # Explicit failure signal\n",
    "\n",
    "# 4. Backtesting Engine\n",
    "def run_detailed_backtest(price_data, market_caps, rebalance_freq='Q', transaction_cost=0.001):\n",
    "    \"\"\"Run backtest with cap-weighted, equal-weighted, and CVaR strategies\"\"\"\n",
    "    # Data preparation\n",
    "    price_data = price_data.ffill().bfill()\n",
    "    returns = price_data.pct_change().dropna()\n",
    "    dates = returns.index\n",
    "    n_stocks = len(price_data.columns)\n",
    "    tickers = price_data.columns.tolist()\n",
    "    \n",
    "    # Get proper quarter-end rebalance dates\n",
    "    rebalance_dates = pd.date_range(dates[0], dates[-1], freq=f'{rebalance_freq}-DEC')\n",
    "    \n",
    "    # Initialize weights\n",
    "    # Cap-weighted (normalize market caps, fill missing with equal weight)\n",
    "    cw_weights = (market_caps / market_caps.sum()).fillna(1/n_stocks).values\n",
    "    cw_weights = cw_weights / cw_weights.sum()  # Ensure exact sum to 1\n",
    "    \n",
    "    # Equal-weighted\n",
    "    ew_weights = np.ones(n_stocks) / n_stocks\n",
    "    \n",
    "    # CVaR starts equal-weighted\n",
    "    cvar_weights = ew_weights.copy()\n",
    "\n",
    "    portfolio_values = {\n",
    "        'CW': [1.0],   # Cap-weighted\n",
    "        'EW': [1.0],    # Equal-weighted\n",
    "        'CVaR': [1.0]   # Optimized\n",
    "    }\n",
    "    \n",
    "    weight_history = []\n",
    "    rebalance_flags = []\n",
    "\n",
    "    # Main backtest loop\n",
    "    for i in range(1, len(dates)):\n",
    "        date = dates[i]\n",
    "        current_prices = price_data.iloc[i]\n",
    "        prev_prices = price_data.iloc[i-1]\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        price_ratios = current_prices / prev_prices\n",
    "        cw_return = np.dot(cw_weights, price_ratios - 1)\n",
    "        ew_return = np.dot(ew_weights, price_ratios - 1)\n",
    "        cvar_return = np.dot(cvar_weights, price_ratios - 1)\n",
    "        \n",
    "        print(f\"date: {date.strftime('%Y-%m-%d')} cw_return: {cw_return:.4f} ew_return: {ew_return:.4f} cvar_return: {cvar_return:.4f}\")\n",
    "\n",
    "        # Check for rebalance\n",
    "        rebalance_flag = 0\n",
    "        if date in rebalance_dates:\n",
    "            lookback_returns = returns.loc[:date].iloc[-252:]  # 1 year lookback\n",
    "            \n",
    "            if len(lookback_returns) > 60:  # Sufficient data\n",
    "                new_weights = optimize_cvar_portfolio(lookback_returns)\n",
    "                \n",
    "                if new_weights is not None:\n",
    "                    rebalance_flag = 1\n",
    "                    # Apply transaction costs\n",
    "                    turnover = np.sum(np.abs(new_weights - cvar_weights))\n",
    "                    portfolio_values['CVaR'][-1] *= (1 - transaction_cost * turnover)\n",
    "                    cvar_weights = new_weights\n",
    "                    print(f\"Successful rebalance on {date.strftime('%Y-%m-%d')}\")\n",
    "                else:\n",
    "                    print(f\"Optimization failed on {date.strftime('%Y-%m-%d')} - maintaining current weights\")\n",
    "        \n",
    "        # Update portfolio values\n",
    "        portfolio_values['CW'].append(portfolio_values['CW'][-1] * (1 + cw_return))\n",
    "        portfolio_values['EW'].append(portfolio_values['EW'][-1] * (1 + ew_return))\n",
    "        portfolio_values['CVaR'].append(portfolio_values['CVaR'][-1] * (1 + cvar_return))\n",
    "        \n",
    "        # Record weights and flags\n",
    "        weight_record = {'Date': date}\n",
    "        for j, ticker in enumerate(tickers):\n",
    "            weight_record[f\"{ticker}_CW\"] = cw_weights[j]\n",
    "            weight_record[f\"{ticker}_CVaR\"] = cvar_weights[j]\n",
    "        weight_history.append(weight_record)\n",
    "        rebalance_flags.append(rebalance_flag)\n",
    "        \n",
    "        # Update drifting weights\n",
    "        cw_weights = (cw_weights * price_ratios)\n",
    "        cw_weights /= cw_weights.sum()\n",
    "        \n",
    "        ew_weights = (ew_weights * price_ratios)\n",
    "        ew_weights /= ew_weights.sum()\n",
    "\n",
    "    # Create results DataFrames\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates[1:],\n",
    "        'CW_Value': portfolio_values['CW'][1:],\n",
    "        'EW_Value': portfolio_values['EW'][1:],\n",
    "        'CVaR_Value': portfolio_values['CVaR'][1:],\n",
    "        'Rebalance_Flag': rebalance_flags\n",
    "    })\n",
    "    \n",
    "    weights_df = pd.DataFrame(weight_history)\n",
    "    \n",
    "    return results_df, weights_df\n",
    "\n",
    "# 5. Performance Metrics\n",
    "def calculate_performance_metrics(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Annualized return\n",
    "    metrics['Annual Return'] = (1 + returns.mean())**252 - 1\n",
    "    \n",
    "    # Annualized volatility\n",
    "    metrics['Annual Volatility'] = returns.std() * np.sqrt(252)\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    metrics['Sharpe Ratio'] = (metrics['Annual Return'] - risk_free_rate) / metrics['Annual Volatility']\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    peak = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - peak) / peak\n",
    "    metrics['Max Drawdown'] = drawdown.min()\n",
    "    \n",
    "    # CVaR\n",
    "    metrics['95% CVaR'] = calculate_cvar(returns)\n",
    "    \n",
    "    # Sortino ratio (using downside deviation)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_dev = downside_returns.std() * np.sqrt(252)\n",
    "    metrics['Sortino Ratio'] = (metrics['Annual Return'] - risk_free_rate) / downside_dev\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2024-12-31'\n",
    "    tickers = [\n",
    "        'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AMD', 'AMZN', 'AVGO', 'AXP', 'BAC', \n",
    "        'BLK', 'BMY', 'BRK-B', 'C', 'CMCSA', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', \n",
    "        'DE', 'DHR', 'DIS', 'FDX', 'GILD', 'GOOG', 'GS', 'HON', 'IBM', 'INTC', \n",
    "        'ISRG', 'JNJ', 'JPM', 'KO', 'LMT', 'MA', 'MCD', 'META', 'MRK', 'MS', \n",
    "        'MSFT', 'NFLX', 'NOW', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PLTR', 'PM', \n",
    "        'PYPL', 'QCOM', 'SCHW', 'T', 'TMUS', 'TSLA', 'TXN', 'V', 'VZ', 'WMT'\n",
    "    ]\n",
    "    \n",
    "    # 1. Fetch data\n",
    "    price_data, market_caps = fetch_stock_data(tickers, start_date, end_date)\n",
    "    \n",
    "    # 2. Run backtest\n",
    "    results_df, weights_df = run_detailed_backtest(price_data, market_caps)\n",
    "    \n",
    "    # 3. Verify initial period\n",
    "    first_rebalance = results_df[results_df['Rebalance_Flag'] == 1].index[0]\n",
    "    initial_period = results_df.iloc[:first_rebalance]\n",
    "    print(f\"Portfolios identical before first rebalance? {np.allclose(initial_period['EW_Value'], initial_period['CVaR_Value'], rtol=1e-02, atol=1e-04)}\")\n",
    "    \n",
    "    # 4. Export results\n",
    "    combined_df = results_df.merge(weights_df, on='Date', how='left')\n",
    "    combined_df['Date'] = combined_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "    combined_df.to_csv('/Volumes/workspace/mixture/etoro_pi/taskA_portfolio_weights_history_and_result.csv', index=False)\n",
    "    \n",
    "    # 5. Plot results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot portfolio values\n",
    "    plt.plot(results_df['Date'], results_df['CW_Value'], label='Cap Weighted', linewidth=2, color='green')\n",
    "    plt.plot(results_df['Date'], results_df['EW_Value'], label='Equal Weighted', linewidth=2, color='blue')\n",
    "    plt.plot(results_df['Date'], results_df['CVaR_Value'], label='CVaR Optimized', linewidth=2, color='orange')\n",
    "    \n",
    "    # Mark rebalance dates\n",
    "    rebalance_dates = results_df[results_df['Rebalance_Flag'] == 1]['Date']\n",
    "    for date in rebalance_dates:\n",
    "        plt.axvline(date, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Highlight first rebalance\n",
    "    plt.axvline(results_df.loc[first_rebalance, 'Date'], color='red', linestyle=':', \n",
    "               label='First Rebalance')\n",
    "    \n",
    "    plt.title('Portfolio Performance Comparison', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Portfolio Value ($1 Initial)', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/Volumes/workspace/mixture/etoro_pi/taskA_portfolio_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Calculate and display metrics\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate returns from portfolio values\n",
    "    cw_returns = results_df['CW_Value'].pct_change().dropna()\n",
    "    ew_returns = results_df['EW_Value'].pct_change().dropna()\n",
    "    cvar_returns = results_df['CVaR_Value'].pct_change().dropna()\n",
    "    \n",
    "    print(\"\\nCap Weighted Strategy:\")\n",
    "    cw_metrics = calculate_performance_metrics(cw_returns)\n",
    "    for k, v in cw_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nEqual Weighted Strategy:\")\n",
    "    ew_metrics = calculate_performance_metrics(ew_returns)\n",
    "    for k, v in ew_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nCVaR Optimized Strategy:\")\n",
    "    cvar_metrics = calculate_performance_metrics(cvar_returns)\n",
    "    for k, v in cvar_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    with open('/Volumes/workspace/mixture/etoro_pi/taskA_performance_metrics.txt', 'w') as f:\n",
    "        f.write(\"Cap Weighted Strategy:\\n\")\n",
    "        for k, v in cw_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nEqual Weighted Strategy:\\n\")\n",
    "        for k, v in ew_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nCVaR Optimized Strategy:\\n\")\n",
    "        for k, v in cvar_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete. Files saved:\")\n",
    "    print(\"- portfolio_weights_history_and_result.csv\")\n",
    "    print(\"- portfolio_comparison.png\")\n",
    "    print(\"- performance_metrics.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee1e5e8b-12b3-411f-a717-eaa3c7db7b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# task B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dd03eb-9a1d-40eb-b1cf-15b2f78e5be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install hmmlearn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b106f0e-1eae-40ec-bd98-def4c5b39af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import cvxpy as cp\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn import hmm\n",
    "import requests\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Enhanced Data Collection Function   \n",
    "def fetch_economic_data(start_date, end_date):\n",
    "    \"\"\"Fetch expanded macroeconomic dataset with GDP growth and Fed rates\"\"\"\n",
    "    fred_api_key = \"c4f4a36ec3edf3db66a7a28b639c5e9b\"  # REQUIRED - get from FRED\n",
    "    \n",
    "    # Expanded series list including GDP and Fed rates\n",
    "    series_ids = {\n",
    "        # Original series\n",
    "        'T10Y2Y': 'T10Y2Y',       # 10-2 Year Treasury Yield Spread\n",
    "        'USREC': 'USREC',          # US Recession Indicators\n",
    "        'VIXCLS': 'VIXCLS',        # VIX Index (fallback to Yahoo)\n",
    "        'DTWEXB': 'DTWEXB',        # Trade Weighted US Dollar Index\n",
    "        'CPIAUCSL': 'CPIAUCSL',    # CPI All Items\n",
    "        # New additions\n",
    "        'GDPC1': 'GDPC1',          # Real GDP (Quarterly)\n",
    "        'GDPC1_PCT': 'A191RL1Q225SBEA', # GDP growth % (Quarterly)\n",
    "        'FEDFUNDS': 'FEDFUNDS',    # Federal Funds Rate\n",
    "        'DFF': 'DFF',              # Daily Federal Funds Rate\n",
    "    }\n",
    "    \n",
    "    econ_data = pd.DataFrame()\n",
    "    \n",
    "    for name, series_id in series_ids.items():\n",
    "        url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={fred_api_key}&file_type=json&observation_start={start_date}&observation_end={end_date}\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if not data.get('observations'):\n",
    "                print(f\"⚠️ No FRED data for {name} ({series_id})\")\n",
    "                continue\n",
    "                \n",
    "            temp_df = pd.DataFrame(data['observations'])\n",
    "            temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "            temp_df = temp_df.dropna(subset=['date', 'value'])\n",
    "            \n",
    "            # Convert quarterly GDP to monthly (forward fill)\n",
    "            if series_id in ['GDPC1', 'A191RL1Q225SBEA']:\n",
    "                temp_df = temp_df.set_index('date')['value'].astype(float).rename(name)\n",
    "                temp_df = temp_df.resample('D').ffill().rename(name)\n",
    "            else:\n",
    "                temp_df = temp_df.set_index('date')['value'].astype(float).rename(name)\n",
    "            \n",
    "            econ_data = pd.concat([econ_data, temp_df], axis=1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"🚨 Error fetching {name}: {str(e)}\")\n",
    "    \n",
    "    # Fallback for VIX if FRED fails\n",
    "    if 'VIXCLS' not in econ_data.columns:\n",
    "        print(\"Fetching VIX from Yahoo Finance...\")\n",
    "        try:\n",
    "            vix = yf.download('^VIX', start=start_date, end=end_date)['Close'].rename('VIXCLS')\n",
    "            econ_data = pd.concat([econ_data, vix], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch VIX: {str(e)}\")\n",
    "    \n",
    "    # Calculate monthly returns for GDP growth if we got quarterly data\n",
    "    if 'GDPC1' in econ_data.columns and 'GDPC1_PCT' not in econ_data.columns:\n",
    "        econ_data['GDPC1_PCT'] = econ_data['GDPC1'].pct_change(periods=1) * 100  # Quarterly growth\n",
    "    \n",
    "    # Forward fill quarterly/daily data to get monthly values\n",
    "    econ_data = econ_data.ffill()\n",
    "    \n",
    "    # Ensure we have all dates in range\n",
    "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    econ_data = econ_data.reindex(full_date_range)\n",
    "    econ_data = econ_data.ffill().bfill()\n",
    "    \n",
    "    return econ_data\n",
    "\n",
    "\n",
    "\n",
    "def fetch_stock_data(tickers, start_date, end_date):\n",
    "    \"\"\"Fetch historical stock data and market caps via Yahoo Finance API\"\"\"\n",
    "    print(f\"Downloading price data for {len(tickers)} stocks...\")\n",
    "    # Get price data\n",
    "    price_data = yf.download(tickers, start=start_date, end=end_date)['Close']\n",
    "    \n",
    "    # Get latest market caps (for initial weights)\n",
    "    print(\"Fetching market cap data...\")\n",
    "    market_caps = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            # Use most recent quarter's market cap\n",
    "            market_cap = stock.info.get('marketCap', np.nan)\n",
    "            market_caps[ticker] = market_cap\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't get market cap for {ticker}: {str(e)}\")\n",
    "            market_caps[ticker] = np.nan\n",
    "    \n",
    "    return price_data, pd.Series(market_caps)\n",
    "\n",
    "# 2. Enhanced CVaR Calculation with Regime Adjustment\n",
    "class RegimeCVaRModel:\n",
    "    def __init__(self, n_regimes=3):\n",
    "        self.n_regimes = n_regimes\n",
    "        self.scaler = StandardScaler()\n",
    "        self.hmm = hmm.GaussianHMM(n_components=n_regimes, \n",
    "                                 covariance_type=\"diag\", \n",
    "                                 n_iter=1000)\n",
    "        self.regime_classifier = None\n",
    "        self.feature_importance = None\n",
    "        self.shap_explainer = None\n",
    "    \n",
    "    def fit_regime_model(self, econ_data):\n",
    "        \"\"\"Fit HMM model to identify market regimes\"\"\"\n",
    "        scaled_data = self.scaler.fit_transform(econ_data)\n",
    "        self.hmm.fit(scaled_data)\n",
    "        \n",
    "        regimes = self.hmm.predict(scaled_data)\n",
    "        self.regime_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.regime_classifier.fit(econ_data, regimes)\n",
    "        \n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': econ_data.columns,\n",
    "            'importance': self.regime_classifier.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        self.shap_explainer = shap.TreeExplainer(self.regime_classifier)\n",
    "        return regimes\n",
    "    \n",
    "    def get_current_regime(self, econ_data):\n",
    "        \"\"\"Predict current market regime\"\"\"\n",
    "        if len(econ_data.shape) == 1:\n",
    "            econ_data = econ_data.values.reshape(1, -1)\n",
    "        scaled_data = self.scaler.transform(econ_data)\n",
    "        return self.hmm.predict(scaled_data)[0]\n",
    "    \n",
    "    def explain_regime(self, econ_data):\n",
    "        \"\"\"Generate SHAP values for regime explanation\"\"\"\n",
    "        if len(econ_data.shape) == 1:\n",
    "            econ_data = econ_data.values.reshape(1, -1)\n",
    "        return self.shap_explainer.shap_values(econ_data)\n",
    "\n",
    "def calculate_cvar(returns, alpha=0.95):\n",
    "    \"\"\"Calculate Conditional Value-at-Risk (CVaR)\"\"\"\n",
    "    var = np.percentile(returns, 100*(1-alpha))\n",
    "    return returns[returns <= var].mean()\n",
    "\n",
    "\n",
    "# 3. Enhanced Portfolio Optimization with Regime Adaptation\n",
    "def optimize_cvar_portfolio(returns, regime=None, alpha=0.95, max_weight=0.050, min_weight=0.001):\n",
    "    \"\"\"Robust optimization with regime adaptation\"\"\"\n",
    "    n = returns.shape[1]\n",
    "    returns = returns.dropna()\n",
    "    \n",
    "    if len(returns) < 60:\n",
    "        print(\"Insufficient data for optimization\")\n",
    "        return None\n",
    "    \n",
    "    weights = cp.Variable(n)\n",
    "    tau = cp.Variable()\n",
    "    portfolio_returns = returns.values @ weights\n",
    "    loss = -portfolio_returns\n",
    "    cvar = tau + (1/(1-alpha)) * cp.mean(cp.pos(loss - tau))\n",
    "    \n",
    "    # Adjust constraints based on regime\n",
    "    if regime == 0:  # High volatility regime\n",
    "        constraints = [\n",
    "            cp.sum(weights) <= 1 + 1e-2,\n",
    "            cp.sum(weights) >= 1 - 1e-2,\n",
    "            weights >= min_weight,\n",
    "            weights <= max_weight * 0.7,\n",
    "            cp.norm(weights, 1) <= 1.2\n",
    "        ]\n",
    "    elif regime == 1:  # Normal regime\n",
    "        constraints = [\n",
    "            cp.sum(weights) <= 1 + 1e-2,\n",
    "            cp.sum(weights) >= 1 - 1e-2,\n",
    "            weights >= min_weight,\n",
    "            weights <= max_weight\n",
    "        ]\n",
    "    else:  # Low volatility regime\n",
    "        constraints = [\n",
    "            cp.sum(weights) <= 1 + 1e-2,\n",
    "            cp.sum(weights) >= 1 - 1e-2,\n",
    "            weights >= min_weight,\n",
    "            weights <= max_weight * 1.2,\n",
    "            cp.norm(weights, 1) <= 1.3\n",
    "        ]\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(cvar), constraints)\n",
    "    \n",
    "    # Try Clarabel first\n",
    "    try:\n",
    "        prob.solve(solver=cp.CLARABEL)\n",
    "        if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            optimized_weights = np.maximum(weights.value, 0)\n",
    "            return optimized_weights / optimized_weights.sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Clarabel failed: {str(e)}\")\n",
    "    \n",
    "    # Fallback to ECOS\n",
    "    try:\n",
    "        prob.solve(solver=cp.ECOS)\n",
    "        if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "            optimized_weights = np.maximum(weights.value, 0)\n",
    "            return optimized_weights / optimized_weights.sum()\n",
    "    except Exception as e:\n",
    "        print(f\"ECOS failed: {str(e)}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 4. Enhanced Backtesting Engine with Walk-Forward Validation\n",
    "def run_enhanced_backtest(price_data, market_caps, econ_data, rebalance_freq='Q', transaction_cost=0.001):\n",
    "    \"\"\"Run backtest with regime adaptation\"\"\"\n",
    "    price_data = price_data.ffill().bfill()\n",
    "    returns = price_data.pct_change().dropna()\n",
    "    dates = returns.index\n",
    "    n_stocks = len(price_data.columns)\n",
    "    tickers = price_data.columns.tolist()\n",
    "    \n",
    "    rebalance_dates = pd.date_range(dates[0], dates[-1], freq=f'{rebalance_freq}-DEC')\n",
    "    \n",
    "    # Initialize regime model\n",
    "    regime_model = RegimeCVaRModel(n_regimes=3)\n",
    "    \n",
    "    # Initialize weights\n",
    "    cw_weights = (market_caps / market_caps.sum()).fillna(1/n_stocks).values\n",
    "    cw_weights /= cw_weights.sum()\n",
    "    ew_weights = np.ones(n_stocks) / n_stocks\n",
    "    cvar_weights = ew_weights.copy()\n",
    "\n",
    "    portfolio_values = {\n",
    "        'CW': [1.0],\n",
    "        'EW': [1.0],\n",
    "        'CVaR': [1.0],\n",
    "        'Enhanced_CVaR': [1.0]\n",
    "    }\n",
    "    \n",
    "    weight_history = []\n",
    "    rebalance_flags = []\n",
    "    regime_history = []\n",
    "    \n",
    "    # Main backtest loop\n",
    "    for i in tqdm(range(1, len(dates)), desc=\"Running backtest\"):\n",
    "        date = dates[i]\n",
    "        current_prices = price_data.iloc[i]\n",
    "        prev_prices = price_data.iloc[i-1]\n",
    "        \n",
    "        price_ratios = current_prices / prev_prices\n",
    "        cw_return = np.dot(cw_weights, price_ratios - 1)\n",
    "        ew_return = np.dot(ew_weights, price_ratios - 1)\n",
    "        cvar_return = np.dot(cvar_weights, price_ratios - 1)\n",
    "        enhanced_cvar_return = cvar_return\n",
    "        \n",
    "        rebalance_flag = 0\n",
    "        if date in rebalance_dates:\n",
    "            lookback_returns = returns.loc[:date].iloc[-252:]\n",
    "            \n",
    "            if len(lookback_returns) > 60:\n",
    "                new_weights = optimize_cvar_portfolio(lookback_returns)\n",
    "                \n",
    "                if date >= pd.to_datetime('2020-01-01'):\n",
    "                    current_econ = econ_data.loc[:date].iloc[-1]\n",
    "                    regime = regime_model.get_current_regime(current_econ)\n",
    "                    enhanced_weights = optimize_cvar_portfolio(lookback_returns, regime=regime)\n",
    "                    \n",
    "                    if enhanced_weights is not None:\n",
    "                        turnover = np.sum(np.abs(enhanced_weights - cvar_weights))\n",
    "                        portfolio_values['Enhanced_CVaR'][-1] *= (1 - transaction_cost * turnover)\n",
    "                        cvar_weights = enhanced_weights\n",
    "                        rebalance_flag = 1\n",
    "                        regime_history.append(regime)\n",
    "                else:\n",
    "                    train_econ = econ_data.loc[:date]\n",
    "                    regimes = regime_model.fit_regime_model(train_econ)\n",
    "                \n",
    "                if new_weights is not None:\n",
    "                    turnover = np.sum(np.abs(new_weights - cvar_weights))\n",
    "                    portfolio_values['CVaR'][-1] *= (1 - transaction_cost * turnover)\n",
    "                    cvar_weights = new_weights\n",
    "                    rebalance_flag = 1\n",
    "        \n",
    "        portfolio_values['CW'].append(portfolio_values['CW'][-1] * (1 + cw_return))\n",
    "        portfolio_values['EW'].append(portfolio_values['EW'][-1] * (1 + ew_return))\n",
    "        portfolio_values['CVaR'].append(portfolio_values['CVaR'][-1] * (1 + cvar_return))\n",
    "        portfolio_values['Enhanced_CVaR'].append(portfolio_values['Enhanced_CVaR'][-1] * (1 + enhanced_cvar_return))\n",
    "        \n",
    "        weight_record = {'Date': date}\n",
    "        for j, ticker in enumerate(tickers):\n",
    "            weight_record[f\"{ticker}_CW\"] = cw_weights[j]\n",
    "            weight_record[f\"{ticker}_CVaR\"] = cvar_weights[j]\n",
    "        weight_history.append(weight_record)\n",
    "        rebalance_flags.append(rebalance_flag)\n",
    "        \n",
    "        cw_weights = (cw_weights * price_ratios)\n",
    "        cw_weights /= cw_weights.sum()\n",
    "        ew_weights = (ew_weights * price_ratios)\n",
    "        ew_weights /= ew_weights.sum()\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': dates[1:],\n",
    "        'CW_Value': portfolio_values['CW'][1:],\n",
    "        'EW_Value': portfolio_values['EW'][1:],\n",
    "        'CVaR_Value': portfolio_values['CVaR'][1:],\n",
    "        'Enhanced_CVaR_Value': portfolio_values['Enhanced_CVaR'][1:],\n",
    "        'Rebalance_Flag': rebalance_flags\n",
    "    })\n",
    "    \n",
    "    weights_df = pd.DataFrame(weight_history)\n",
    "    \n",
    "    return results_df, weights_df, regime_model\n",
    "\n",
    "# 5. Performance Metrics (unchanged)\n",
    "def calculate_performance_metrics(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Annualized return\n",
    "    metrics['Annual Return'] = (1 + returns.mean())**252 - 1\n",
    "    \n",
    "    # Annualized volatility\n",
    "    metrics['Annual Volatility'] = returns.std() * np.sqrt(252)\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    metrics['Sharpe Ratio'] = (metrics['Annual Return'] - risk_free_rate) / metrics['Annual Volatility']\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    peak = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - peak) / peak\n",
    "    metrics['Max Drawdown'] = drawdown.min()\n",
    "    \n",
    "    # CVaR\n",
    "    metrics['95% CVaR'] = calculate_cvar(returns)\n",
    "    \n",
    "    # Sortino ratio\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_dev = downside_returns.std() * np.sqrt(252)\n",
    "    metrics['Sortino Ratio'] = (metrics['Annual Return'] - risk_free_rate) / downside_dev\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2024-12-31'\n",
    "    tickers = [\n",
    "        'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AMD', 'AMZN', 'AVGO', 'AXP', 'BAC', \n",
    "        'BLK', 'BMY', 'BRK-B', 'C', 'CMCSA', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', \n",
    "        'DE', 'DHR', 'DIS', 'FDX', 'GILD', 'GOOG', 'GS', 'HON', 'IBM', 'INTC', \n",
    "        'ISRG', 'JNJ', 'JPM', 'KO', 'LMT', 'MA', 'MCD', 'META', 'MRK', 'MS', \n",
    "        'MSFT', 'NFLX', 'NOW', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PLTR', 'PM', \n",
    "        'PYPL', 'QCOM', 'SCHW', 'T', 'TMUS', 'TSLA', 'TXN', 'V', 'VZ', 'WMT'\n",
    "    ]\n",
    "    \n",
    "    # 1. Fetch data\n",
    "    print(\"Fetching market data...\")\n",
    "    price_data, market_caps = fetch_stock_data(tickers, start_date, end_date)\n",
    "    print(\"Fetching economic data...\")\n",
    "    econ_data = fetch_economic_data(start_date, end_date)\n",
    "    \n",
    "    # 2. Run enhanced backtest\n",
    "    print(\"Running enhanced backtest...\")\n",
    "    results_df, weights_df, regime_model = run_enhanced_backtest(price_data, market_caps, econ_data)\n",
    "    \n",
    "\t\n",
    "    # 3. Verify initial period\n",
    "    first_rebalance = results_df[results_df['Rebalance_Flag'] == 1].index[0]\n",
    "    initial_period = results_df.iloc[:first_rebalance]\n",
    "    print(f\"Portfolios identical before first rebalance? {np.allclose(initial_period['EW_Value'], initial_period['CVaR_Value'], rtol=1e-02, atol=1e-04)}\")\n",
    "    \n",
    "    # 4. Export results\n",
    "    combined_df = results_df.merge(weights_df, on='Date', how='left')\n",
    "    combined_df['Date'] = combined_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "    combined_df.to_csv('/Volumes/workspace/mixture/etoro_pi/taskB_enhanced_portfolio_results.csv', index=False)\n",
    "    \n",
    "    # 5. Plot results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot portfolio values\n",
    "    plt.plot(results_df['Date'], results_df['CW_Value'], label='Cap Weighted', linewidth=2, color='green')\n",
    "    plt.plot(results_df['Date'], results_df['EW_Value'], label='Equal Weighted', linewidth=2, color='blue')\n",
    "    plt.plot(results_df['Date'], results_df['CVaR_Value'], label='Standard CVaR', linewidth=2, color='orange')\n",
    "    plt.plot(results_df['Date'], results_df['Enhanced_CVaR_Value'], label='Enhanced CVaR', linewidth=2, color='red')\n",
    "    \n",
    "    # Mark test period start\n",
    "    plt.axvline(pd.to_datetime('2020-01-01'), color='purple', linestyle='--', \n",
    "               label='Test Period Start')\n",
    "    \n",
    "    plt.title('Enhanced Portfolio Performance Comparison', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Portfolio Value ($1 Initial)', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Format x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/Volumes/workspace/mixture/etoro_pi/taskB_enhanced_portfolio_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Calculate and display metrics for test period only\n",
    "    test_results = results_df[results_df['Date'] >= '2020-01-01']\n",
    "    \n",
    "    print(\"\\nPerformance Metrics (Jan 2020 - Dec 2024):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate returns from portfolio values\n",
    "    cw_returns = test_results['CW_Value'].pct_change().dropna()\n",
    "    ew_returns = test_results['EW_Value'].pct_change().dropna()\n",
    "    cvar_returns = test_results['CVaR_Value'].pct_change().dropna()\n",
    "    enhanced_returns = test_results['Enhanced_CVaR_Value'].pct_change().dropna()\n",
    "    \n",
    "    print(\"\\nCap Weighted Strategy:\")\n",
    "    cw_metrics = calculate_performance_metrics(cw_returns)\n",
    "    for k, v in cw_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nEqual Weighted Strategy:\")\n",
    "    ew_metrics = calculate_performance_metrics(ew_returns)\n",
    "    for k, v in ew_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nStandard CVaR Strategy:\")\n",
    "    cvar_metrics = calculate_performance_metrics(cvar_returns)\n",
    "    for k, v in cvar_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nEnhanced CVaR Strategy:\")\n",
    "    enhanced_metrics = calculate_performance_metrics(enhanced_returns)\n",
    "    for k, v in enhanced_metrics.items():\n",
    "        print(f\"{k:>20}: {v:.4f}\")\n",
    "    \n",
    "    # 7. Save regime model analysis\n",
    "    print(\"\\nRegime Model Feature Importance:\")\n",
    "    print(regime_model.feature_importance)\n",
    "    \n",
    "    \n",
    "    # Save metrics\n",
    "    with open('/Volumes/workspace/mixture/etoro_pi/taskB_enhanced_performance_metrics.txt', 'w') as f:\n",
    "        f.write(\"Cap Weighted Strategy:\\n\")\n",
    "        for k, v in cw_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nEqual Weighted Strategy:\\n\")\n",
    "        for k, v in ew_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nStandard CVaR Strategy:\\n\")\n",
    "        for k, v in cvar_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nEnhanced CVaR Strategy:\\n\")\n",
    "        for k, v in enhanced_metrics.items():\n",
    "            f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nRegime Model Feature Importance:\\n\")\n",
    "        f.write(regime_model.feature_importance.to_string())\n",
    "    \n",
    "    print(\"\\nAnalysis complete. Files saved:\")\n",
    "    print(\"- enhanced_portfolio_results.csv\")\n",
    "    print(\"- enhanced_portfolio_comparison.png\")\n",
    "    print(\"- enhanced_performance_metrics.txt\")\n",
    "    print(\"- regime_shap_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fbca75-0b03-41be-8ee7-c892717a0dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc92aa3c-9375-44e5-8dcb-585f49653815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install textblob hmmlearn shap feedparser pytrends nltk\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4feacf68-3ee3-4ac9-bfa9-e86e65882c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Import required libraries\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "# import cvxpy as cp\n",
    "# from datetime import datetime, timedelta\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.dates as mdates\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from hmmlearn import hmm\n",
    "# import requests\n",
    "# import json\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import shap\n",
    "# from tqdm import tqdm\n",
    "# from textblob import TextBlob\n",
    "# import feedparser\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# from dateutil import parser as date_parser\n",
    "\n",
    "# # 1. Enhanced Data Collection Function   \n",
    "# def fetch_economic_data(start_date, end_date):\n",
    "#     \"\"\"Fetch expanded macroeconomic dataset with GDP growth and Fed rates (robust version)\"\"\"\n",
    "#     fred_api_key = \"c4f4a36ec3edf3db66a7a28b639c5e9b\"  # REQUIRED - get from FRED\n",
    "    \n",
    "#     # Expanded series list including GDP and Fed rates\n",
    "#     series_ids = {\n",
    "#         'T10Y2Y': 'T10Y2Y',        # 10-2 Year Treasury Yield Spread\n",
    "#         'USREC': 'USREC',          # US Recession Indicators\n",
    "#         'VIXCLS': 'VIXCLS',        # VIX Index (fallback to Yahoo)\n",
    "#         'DTWEXB': 'DTWEXB',        # Trade Weighted US Dollar Index\n",
    "#         'CPIAUCSL': 'CPIAUCSL',    # CPI All Items\n",
    "#         'GDPC1': 'GDPC1',          # Real GDP (Quarterly)\n",
    "#         'GDPC1_PCT': 'A191RL1Q225SBEA',  # GDP growth % (Quarterly)\n",
    "#         'FEDFUNDS': 'FEDFUNDS',    # Federal Funds Rate\n",
    "#         'DFF': 'DFF',              # Daily Federal Funds Rate\n",
    "#     }\n",
    "\n",
    "#     econ_data = pd.DataFrame()\n",
    "\n",
    "#     for name, series_id in series_ids.items():\n",
    "#         url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={fred_api_key}&file_type=json&observation_start={start_date}&observation_end={end_date}\"\n",
    "#         try:\n",
    "#             response = requests.get(url, timeout=10)\n",
    "#             response.raise_for_status()\n",
    "#             data = response.json()\n",
    "\n",
    "#             if not data.get('observations'):\n",
    "#                 print(f\"⚠️ No FRED data for {name} ({series_id})\")\n",
    "#                 continue\n",
    "\n",
    "#             temp_df = pd.DataFrame(data['observations'])\n",
    "#             temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "#             temp_df['value'] = pd.to_numeric(temp_df['value'], errors='coerce')\n",
    "#             temp_df = temp_df.dropna(subset=['value'])\n",
    "            \n",
    "#             if series_id in ['GDPC1', 'A191RL1Q225SBEA']:\n",
    "#                 temp_df = temp_df.set_index('date')['value'].resample('D').ffill().rename(name)\n",
    "#             else:\n",
    "#                 temp_df = temp_df.set_index('date')['value'].rename(name)\n",
    "\n",
    "#             econ_data = pd.concat([econ_data, temp_df], axis=1)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"🚨 Error fetching {name}: {str(e)}\")\n",
    "\n",
    "#     # Fallback for VIX if FRED fails\n",
    "#     if 'VIXCLS' not in econ_data.columns:\n",
    "#         print(\"Fetching VIX from Yahoo Finance...\")\n",
    "#         try:\n",
    "#             vix = yf.download('^VIX', start=start_date, end=end_date)['Close'].rename('VIXCLS')\n",
    "#             econ_data = pd.concat([econ_data, vix], axis=1)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to fetch VIX: {str(e)}\")\n",
    "\n",
    "#     # Calculate GDP growth % if needed\n",
    "#     if 'GDPC1' in econ_data.columns and 'GDPC1_PCT' not in econ_data.columns:\n",
    "#         econ_data['GDPC1_PCT'] = econ_data['GDPC1'].pct_change() * 100\n",
    "\n",
    "#     # Final processing and validation\n",
    "#     econ_data = econ_data.ffill().bfill()\n",
    "#     full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "#     econ_data = econ_data.reindex(full_date_range).ffill().bfill()\n",
    "    \n",
    "#     if econ_data.empty:\n",
    "#         raise ValueError(\"No economic data was successfully fetched\")\n",
    "    \n",
    "#     valid_columns = econ_data.columns[econ_data.notna().any()]\n",
    "#     if len(valid_columns) < 3:\n",
    "#         raise ValueError(f\"Insufficient economic data. Only {len(valid_columns)} columns have data\")\n",
    "    \n",
    "#     return econ_data[valid_columns]\n",
    "\n",
    "# def fetch_stock_data(tickers, start_date, end_date):\n",
    "#     \"\"\"Fetch historical stock data and market caps via Yahoo Finance API\"\"\"\n",
    "#     print(f\"Downloading price data for {len(tickers)} stocks...\")\n",
    "#     price_data = yf.download(tickers, start=start_date, end=end_date)['Close']\n",
    "    \n",
    "#     print(\"Fetching market cap data...\")\n",
    "#     market_caps = {}\n",
    "#     for ticker in tickers:\n",
    "#         try:\n",
    "#             stock = yf.Ticker(ticker)\n",
    "#             market_cap = stock.info.get('marketCap', np.nan)\n",
    "#             market_caps[ticker] = market_cap\n",
    "#         except Exception as e:\n",
    "#             print(f\"Couldn't get market cap for {ticker}: {str(e)}\")\n",
    "#             market_caps[ticker] = np.nan\n",
    "    \n",
    "#     return price_data, pd.Series(market_caps)\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"Clean text for sentiment analysis\"\"\"\n",
    "#     text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "#     return text.lower()\n",
    "\n",
    "# def fetch_yahoo_news_sentiment(tickers, start_date, end_date):\n",
    "#     \"\"\"Fetch news sentiment from Yahoo Finance RSS feeds\"\"\"\n",
    "#     base_url = \"https://finance.yahoo.com/rss/headline?s=\"\n",
    "#     sentiment_data = pd.DataFrame()\n",
    "    \n",
    "#     start_date = pd.to_datetime(start_date).date()\n",
    "#     end_date = pd.to_datetime(end_date).date()\n",
    "    \n",
    "#     for ticker in tqdm(tickers, desc=\"Fetching Yahoo news sentiment\"):\n",
    "#         try:\n",
    "#             url = f\"{base_url}{ticker}\"\n",
    "#             feed = feedparser.parse(url)\n",
    "            \n",
    "#             sentiments = []\n",
    "#             dates = []\n",
    "            \n",
    "#             for entry in feed.entries:\n",
    "#                 try:\n",
    "#                     pub_date = date_parser.parse(entry.published).date()\n",
    "#                     if not (start_date <= pub_date <= end_date):\n",
    "#                         continue\n",
    "                    \n",
    "#                     title = clean_text(entry.title)\n",
    "#                     description = clean_text(entry.description) if hasattr(entry, 'description') else \"\"\n",
    "#                     text = f\"{title}. {description}\"\n",
    "                    \n",
    "#                     blob = TextBlob(text)\n",
    "#                     sentiment = blob.sentiment.polarity\n",
    "                    \n",
    "#                     sentiments.append(sentiment)\n",
    "#                     dates.append(pub_date)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing entry for {ticker}: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             if sentiments:\n",
    "#                 temp_df = pd.DataFrame({'date': dates, ticker: sentiments})\n",
    "#                 temp_df = temp_df.groupby('date').mean()\n",
    "#                 sentiment_data = pd.concat([sentiment_data, temp_df], axis=1)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing feed for {ticker}: {str(e)}\")\n",
    "#             continue\n",
    "    \n",
    "#     if not sentiment_data.empty:\n",
    "#         full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "#         sentiment_data = sentiment_data.reindex(full_date_range)\n",
    "#         sentiment_data = sentiment_data.ffill().bfill()\n",
    "        \n",
    "#         valid_columns = [col for col in sentiment_data.columns if col in tickers]\n",
    "#         if valid_columns:\n",
    "#             sentiment_data['MARKET_SENTIMENT'] = sentiment_data[valid_columns].mean(axis=1)\n",
    "#         else:\n",
    "#             sentiment_data['MARKET_SENTIMENT'] = np.nan\n",
    "#     else:\n",
    "#         full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "#         sentiment_data = pd.DataFrame(index=full_date_range)\n",
    "#         sentiment_data['MARKET_SENTIMENT'] = np.nan\n",
    "    \n",
    "#     return sentiment_data\n",
    "\n",
    "# # 2. Enhanced CVaR Calculation with Regime Adjustment\n",
    "# class RegimeCVaRModel:\n",
    "#     def __init__(self, n_regimes=3):\n",
    "#         self.n_regimes = n_regimes\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.hmm = hmm.GaussianHMM(n_components=n_regimes, \n",
    "#                                  covariance_type=\"diag\", \n",
    "#                                  n_iter=1000)\n",
    "#         self.regime_classifier = None\n",
    "#         self.feature_importance = None\n",
    "#         self.shap_explainer = None\n",
    "#         self.sentiment_data = None\n",
    "    \n",
    "#     def add_sentiment_data(self, sentiment_data):\n",
    "#         \"\"\"Add news sentiment data to the model\"\"\"\n",
    "#         self.sentiment_data = sentiment_data\n",
    "    \n",
    "#     def fit_regime_model(self, econ_data, price_data=None):\n",
    "#         \"\"\"Fit HMM model to identify market regimes with optional sentiment data\"\"\"\n",
    "#         if self.sentiment_data is not None and 'MARKET_SENTIMENT' in self.sentiment_data.columns:\n",
    "#             market_sentiment = self.sentiment_data['MARKET_SENTIMENT'].copy()\n",
    "#             combined_data = pd.concat([econ_data, market_sentiment], axis=1)\n",
    "#         else:\n",
    "#             combined_data = econ_data.copy()\n",
    "        \n",
    "#         combined_data = combined_data.ffill().bfill().dropna()\n",
    "        \n",
    "#         if len(combined_data) == 0:\n",
    "#             print(\"Warning: No valid data remaining after cleaning. Using economic data only.\")\n",
    "#             combined_data = econ_data.ffill().bfill().dropna()\n",
    "#             if len(combined_data) == 0:\n",
    "#                 raise ValueError(\"No valid economic data available for regime modeling\")\n",
    "        \n",
    "#         try:\n",
    "#             scaled_data = self.scaler.fit_transform(combined_data)\n",
    "#             self.hmm.fit(scaled_data)\n",
    "            \n",
    "#             regimes = self.hmm.predict(scaled_data)\n",
    "#             self.regime_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#             self.regime_classifier.fit(combined_data, regimes)\n",
    "            \n",
    "#             self.feature_importance = pd.DataFrame({\n",
    "#                 'feature': combined_data.columns,\n",
    "#                 'importance': self.regime_classifier.feature_importances_\n",
    "#             }).sort_values('importance', ascending=False)\n",
    "            \n",
    "#             self.shap_explainer = shap.TreeExplainer(self.regime_classifier)\n",
    "#             return regimes\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fitting regime model: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def get_current_regime(self, econ_data, current_sentiment=None):\n",
    "#         \"\"\"Predict current market regime with optional sentiment\"\"\"\n",
    "#         if current_sentiment is not None:\n",
    "#             if isinstance(current_sentiment, (float, int)):\n",
    "#                 current_sentiment = {'MARKET_SENTIMENT': current_sentiment}\n",
    "#             combined_data = pd.concat([econ_data, pd.Series(current_sentiment)], axis=1)\n",
    "#         else:\n",
    "#             combined_data = econ_data.copy()\n",
    "            \n",
    "#         combined_data = combined_data.ffill().bfill().dropna()\n",
    "        \n",
    "#         if combined_data.empty:\n",
    "#             raise ValueError(\"No valid data available for predicting the current regime\")\n",
    "        \n",
    "#         if len(combined_data.shape) == 1:\n",
    "#             combined_data = combined_data.values.reshape(1, -1)\n",
    "#         elif combined_data.shape[1] != self.scaler.n_features_in_:\n",
    "#             # Ensure the input data has the same number of features as the scaler was trained on\n",
    "#             missing_features = set(self.scaler.feature_names_in_) - set(combined_data.columns)\n",
    "#             for feature in missing_features:\n",
    "#                 combined_data[feature] = 0\n",
    "#             combined_data = combined_data[self.scaler.feature_names_in_]\n",
    "        \n",
    "#         scaled_data = self.scaler.transform(combined_data)\n",
    "#         return self.hmm.predict(scaled_data)[0]\n",
    "    \n",
    "#     def explain_regime(self, econ_data, current_sentiment=None):\n",
    "#         \"\"\"Generate SHAP values for regime explanation\"\"\"\n",
    "#         if current_sentiment is not None:\n",
    "#             if isinstance(current_sentiment, (float, int)):\n",
    "#                 current_sentiment = {'MARKET_SENTIMENT': current_sentiment}\n",
    "#             combined_data = pd.concat([econ_data, pd.Series(current_sentiment)], axis=1)\n",
    "#         else:\n",
    "#             combined_data = econ_data.copy()\n",
    "            \n",
    "#         if len(combined_data.shape) == 1:\n",
    "#             combined_data = combined_data.values.reshape(1, -1)\n",
    "#         return self.shap_explainer.shap_values(combined_data)\n",
    "\n",
    "# def calculate_cvar(returns, alpha=0.95):\n",
    "#     \"\"\"Calculate Conditional Value-at-Risk (CVaR)\"\"\"\n",
    "#     var = np.percentile(returns, 100*(1-alpha))\n",
    "#     return returns[returns <= var].mean()\n",
    "\n",
    "# # 3. Enhanced Portfolio Optimization with Regime Adaptation\n",
    "# def optimize_cvar_portfolio(returns, regime=None, alpha=0.95, max_weight=0.050, min_weight=0.001):\n",
    "#     \"\"\"Robust optimization with regime adaptation\"\"\"\n",
    "#     n = returns.shape[1]\n",
    "#     returns = returns.dropna()\n",
    "    \n",
    "#     if len(returns) < 60:\n",
    "#         print(\"Insufficient data for optimization\")\n",
    "#         return None\n",
    "    \n",
    "#     weights = cp.Variable(n)\n",
    "#     tau = cp.Variable()\n",
    "#     portfolio_returns = returns.values @ weights\n",
    "#     loss = -portfolio_returns\n",
    "#     cvar = tau + (1/(1-alpha)) * cp.mean(cp.pos(loss - tau))\n",
    "    \n",
    "#     # Adjust constraints based on regime\n",
    "#     if regime == 0:  # High volatility regime\n",
    "#         constraints = [\n",
    "#             cp.sum(weights) <= 1 + 1e-2,\n",
    "#             cp.sum(weights) >= 1 - 1e-2,\n",
    "#             weights >= min_weight,\n",
    "#             weights <= max_weight * 0.7,\n",
    "#             cp.norm(weights, 1) <= 1.2\n",
    "#         ]\n",
    "#     elif regime == 1:  # Normal regime\n",
    "#         constraints = [\n",
    "#             cp.sum(weights) <= 1 + 1e-2,\n",
    "#             cp.sum(weights) >= 1 - 1e-2,\n",
    "#             weights >= min_weight,\n",
    "#             weights <= max_weight\n",
    "#         ]\n",
    "#     else:  # Low volatility regime\n",
    "#         constraints = [\n",
    "#             cp.sum(weights) <= 1 + 1e-2,\n",
    "#             cp.sum(weights) >= 1 - 1e-2,\n",
    "#             weights >= min_weight,\n",
    "#             weights <= max_weight * 1.2,\n",
    "#             cp.norm(weights, 1) <= 1.3\n",
    "#         ]\n",
    "    \n",
    "#     prob = cp.Problem(cp.Minimize(cvar), constraints)\n",
    "    \n",
    "#     try:\n",
    "#         prob.solve(solver=cp.CLARABEL)\n",
    "#         if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "#             optimized_weights = np.maximum(weights.value, 0)\n",
    "#             return optimized_weights / optimized_weights.sum()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Clarabel failed: {str(e)}\")\n",
    "    \n",
    "#     try:\n",
    "#         prob.solve(solver=cp.ECOS)\n",
    "#         if prob.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "#             optimized_weights = np.maximum(weights.value, 0)\n",
    "#             return optimized_weights / optimized_weights.sum()\n",
    "#     except Exception as e:\n",
    "#         print(f\"ECOS failed: {str(e)}\")\n",
    "    \n",
    "#     return None\n",
    "\n",
    "# # 4. Enhanced Backtesting Engine with Walk-Forward Validation\n",
    "# def run_enhanced_backtest(price_data, market_caps, econ_data, sentiment_data=None, rebalance_freq='Q', transaction_cost=0.001):\n",
    "#     \"\"\"Run backtest with regime adaptation\"\"\"\n",
    "#     if sentiment_data is not None:\n",
    "#         if sentiment_data.empty or ('MARKET_SENTIMENT' not in sentiment_data.columns):\n",
    "#             print(\"Warning: No valid sentiment data available. Proceeding without sentiment analysis.\")\n",
    "#             sentiment_data = None\n",
    "#         else:\n",
    "#             sentiment_data = sentiment_data[['MARKET_SENTIMENT']].ffill().bfill()\n",
    "    \n",
    "#     price_data = price_data.ffill().bfill()\n",
    "#     returns = price_data.pct_change().dropna()\n",
    "#     dates = returns.index\n",
    "#     n_stocks = len(price_data.columns)\n",
    "#     tickers = price_data.columns.tolist()\n",
    "    \n",
    "#     rebalance_dates = pd.date_range(dates[0], dates[-1], freq=f'{rebalance_freq}-DEC')\n",
    "    \n",
    "#     regime_model = RegimeCVaRModel(n_regimes=3)\n",
    "    \n",
    "#     if sentiment_data is not None:\n",
    "#         regime_model.add_sentiment_data(sentiment_data)\n",
    "    \n",
    "#     cw_weights = (market_caps / market_caps.sum()).fillna(1/n_stocks).values\n",
    "#     cw_weights /= cw_weights.sum()\n",
    "#     ew_weights = np.ones(n_stocks) / n_stocks\n",
    "#     cvar_weights = ew_weights.copy()\n",
    "\n",
    "#     portfolio_values = {\n",
    "#         'CW': [1.0],\n",
    "#         'EW': [1.0],\n",
    "#         'CVaR': [1.0],\n",
    "#         'Enhanced_CVaR': [1.0]\n",
    "#     }\n",
    "    \n",
    "#     weight_history = []\n",
    "#     rebalance_flags = []\n",
    "#     regime_history = []\n",
    "    \n",
    "#     for i in tqdm(range(1, len(dates)), desc=\"Running backtest\"):\n",
    "#         date = dates[i]\n",
    "#         current_prices = price_data.iloc[i]\n",
    "#         prev_prices = price_data.iloc[i-1]\n",
    "        \n",
    "#         price_ratios = current_prices / prev_prices\n",
    "#         cw_return = np.dot(cw_weights, price_ratios - 1)\n",
    "#         ew_return = np.dot(ew_weights, price_ratios - 1)\n",
    "#         cvar_return = np.dot(cvar_weights, price_ratios - 1)\n",
    "#         enhanced_cvar_return = cvar_return\n",
    "        \n",
    "#         rebalance_flag = 0\n",
    "#         if date in rebalance_dates:\n",
    "#             lookback_returns = returns.loc[:date].iloc[-252:]\n",
    "            \n",
    "#             if len(lookback_returns) > 60:\n",
    "#                 new_weights = optimize_cvar_portfolio(lookback_returns)\n",
    "                \n",
    "#                 if date >= pd.to_datetime('2020-01-01'):\n",
    "#                     current_econ = econ_data.loc[:date].iloc[-1]\n",
    "                    \n",
    "#                     current_sentiment = None\n",
    "#                     if sentiment_data is not None and date in sentiment_data.index:\n",
    "#                         current_sentiment = sentiment_data.loc[date, 'MARKET_SENTIMENT']\n",
    "                    \n",
    "#                     regime = regime_model.get_current_regime(current_econ, current_sentiment)\n",
    "#                     enhanced_weights = optimize_cvar_portfolio(lookback_returns, regime=regime)\n",
    "                    \n",
    "#                     if enhanced_weights is not None:\n",
    "#                         turnover = np.sum(np.abs(enhanced_weights - cvar_weights))\n",
    "#                         portfolio_values['Enhanced_CVaR'][-1] *= (1 - transaction_cost * turnover)\n",
    "#                         cvar_weights = enhanced_weights\n",
    "#                         rebalance_flag = 1\n",
    "#                         regime_history.append(regime)\n",
    "#                 else:\n",
    "#                     train_econ = econ_data.loc[:date]\n",
    "#                     regimes = regime_model.fit_regime_model(train_econ)\n",
    "                \n",
    "#                 if new_weights is not None:\n",
    "#                     turnover = np.sum(np.abs(new_weights - cvar_weights))\n",
    "#                     portfolio_values['CVaR'][-1] *= (1 - transaction_cost * turnover)\n",
    "#                     cvar_weights = new_weights\n",
    "#                     rebalance_flag = 1\n",
    "        \n",
    "#         portfolio_values['CW'].append(portfolio_values['CW'][-1] * (1 + cw_return))\n",
    "#         portfolio_values['EW'].append(portfolio_values['EW'][-1] * (1 + ew_return))\n",
    "#         portfolio_values['CVaR'].append(portfolio_values['CVaR'][-1] * (1 + cvar_return))\n",
    "#         portfolio_values['Enhanced_CVaR'].append(portfolio_values['Enhanced_CVaR'][-1] * (1 + enhanced_cvar_return))\n",
    "        \n",
    "#         weight_record = {'Date': date}\n",
    "#         for j, ticker in enumerate(tickers):\n",
    "#             weight_record[f\"{ticker}_CW\"] = cw_weights[j]\n",
    "#             weight_record[f\"{ticker}_CVaR\"] = cvar_weights[j]\n",
    "#         weight_history.append(weight_record)\n",
    "#         rebalance_flags.append(rebalance_flag)\n",
    "        \n",
    "#         cw_weights = (cw_weights * price_ratios)\n",
    "#         cw_weights /= cw_weights.sum()\n",
    "#         ew_weights = (ew_weights * price_ratios)\n",
    "#         ew_weights /= ew_weights.sum()\n",
    "\n",
    "#     results_df = pd.DataFrame({\n",
    "#         'Date': dates[1:],\n",
    "#         'CW_Value': portfolio_values['CW'][1:],\n",
    "#         'EW_Value': portfolio_values['EW'][1:],\n",
    "#         'CVaR_Value': portfolio_values['CVaR'][1:],\n",
    "#         'Enhanced_CVaR_Value': portfolio_values['Enhanced_CVaR'][1:],\n",
    "#         'Rebalance_Flag': rebalance_flags\n",
    "#     })\n",
    "    \n",
    "#     weights_df = pd.DataFrame(weight_history)\n",
    "    \n",
    "#     return results_df, weights_df, regime_model\n",
    "\n",
    "# # 5. Performance Metrics\n",
    "# def calculate_performance_metrics(returns, risk_free_rate=0.0):\n",
    "#     \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "#     metrics = {}\n",
    "    \n",
    "#     metrics['Annual Return'] = (1 + returns.mean())**252 - 1\n",
    "#     metrics['Annual Volatility'] = returns.std() * np.sqrt(252)\n",
    "#     metrics['Sharpe Ratio'] = (metrics['Annual Return'] - risk_free_rate) / metrics['Annual Volatility']\n",
    "    \n",
    "#     cumulative = (1 + returns).cumprod()\n",
    "#     peak = cumulative.expanding().max()\n",
    "#     drawdown = (cumulative - peak) / peak\n",
    "#     metrics['Max Drawdown'] = drawdown.min()\n",
    "    \n",
    "#     metrics['95% CVaR'] = calculate_cvar(returns)\n",
    "    \n",
    "#     downside_returns = returns[returns < 0]\n",
    "#     downside_dev = downside_returns.std() * np.sqrt(252)\n",
    "#     metrics['Sortino Ratio'] = (metrics['Annual Return'] - risk_free_rate) / downside_dev\n",
    "    \n",
    "#     return metrics\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         # Configuration\n",
    "#         start_date = '2010-01-01'\n",
    "#         end_date = '2024-12-31'\n",
    "#         tickers = [\n",
    "#             'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AMD', 'AMZN', 'AVGO', 'AXP', 'BAC', \n",
    "#             'BLK', 'BMY', 'BRK-B', 'C', 'CMCSA', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', \n",
    "#             'DE', 'DHR', 'DIS', 'FDX', 'GILD', 'GOOG', 'GS', 'HON', 'IBM', 'INTC', \n",
    "#             'ISRG', 'JNJ', 'JPM', 'KO', 'LMT', 'MA', 'MCD', 'META', 'MRK', 'MS', \n",
    "#             'MSFT', 'NFLX', 'NOW', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PLTR', 'PM', \n",
    "#             'PYPL', 'QCOM', 'SCHW', 'T', 'TMUS', 'TSLA', 'TXN', 'V', 'VZ', 'WMT'\n",
    "#         ]\n",
    "        \n",
    "#         # 1. Fetch data\n",
    "#         print(\"Fetching market data...\")\n",
    "#         price_data, market_caps = fetch_stock_data(tickers, start_date, end_date)\n",
    "#         print(\"Fetching economic data...\")\n",
    "#         econ_data = fetch_economic_data(start_date, end_date)\n",
    "        \n",
    "#         print(\"Fetching news sentiment from Yahoo RSS...\")\n",
    "#         try:\n",
    "#             sentiment_data = fetch_yahoo_news_sentiment(tickers, start_date, end_date)\n",
    "#             if sentiment_data.empty or 'MARKET_SENTIMENT' not in sentiment_data.columns:\n",
    "#                 print(\"Warning: No valid sentiment data obtained. Continuing without sentiment analysis.\")\n",
    "#                 sentiment_data = None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error fetching sentiment data: {str(e)}. Continuing without sentiment analysis.\")\n",
    "#             sentiment_data = None\n",
    "        \n",
    "#         # 2. Run enhanced backtest\n",
    "#         print(\"Running enhanced backtest...\")\n",
    "#         results_df, weights_df, regime_model = run_enhanced_backtest(\n",
    "#             price_data, market_caps, econ_data, sentiment_data\n",
    "#         )\n",
    "        \n",
    "#         # 3. Verify initial period\n",
    "#         first_rebalance = results_df[results_df['Rebalance_Flag'] == 1].index[0]\n",
    "#         initial_period = results_df.iloc[:first_rebalance]\n",
    "#         print(f\"Portfolios identical before first rebalance? {np.allclose(initial_period['EW_Value'], initial_period['CVaR_Value'], rtol=1e-02, atol=1e-04)}\")\n",
    "        \n",
    "#         # 4. Export results\n",
    "#         combined_df = results_df.merge(weights_df, on='Date', how='left')\n",
    "#         combined_df['Date'] = combined_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "#         combined_df.to_csv('/Volumes/workspace/mixture/etoro_pi/taskC_alpha_enhanced_results.csv', index=False)\n",
    "        \n",
    "#         # 5. Plot results\n",
    "#         plt.figure(figsize=(14, 7))\n",
    "#         plt.plot(results_df['Date'], results_df['CW_Value'], label='Cap Weighted', linewidth=2, color='green')\n",
    "#         plt.plot(results_df['Date'], results_df['EW_Value'], label='Equal Weighted', linewidth=2, color='blue')\n",
    "#         plt.plot(results_df['Date'], results_df['CVaR_Value'], label='Standard CVaR', linewidth=2, color='orange')\n",
    "#         plt.plot(results_df['Date'], results_df['Enhanced_CVaR_Value'], label='Enhanced CVaR', linewidth=2, color='red')\n",
    "#         plt.axvline(pd.to_datetime('2020-01-01'), color='purple', linestyle='--', label='Test Period Start')\n",
    "        \n",
    "#         plt.title('Enhanced Portfolio Performance Comparison', fontsize=16)\n",
    "#         plt.xlabel('Date', fontsize=12)\n",
    "#         plt.ylabel('Portfolio Value ($1 Initial)', fontsize=12)\n",
    "#         plt.legend(fontsize=12)\n",
    "#         plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "#         plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "#         plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "#         plt.gcf().autofmt_xdate()\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('/Volumes/workspace/mixture/etoro_pi/taskC_portfolio_comparison.png', dpi=300)\n",
    "#         plt.show()\n",
    "        \n",
    "#         # 6. Calculate and display metrics for test period only\n",
    "#         test_results = results_df[results_df['Date'] >= '2020-01-01']\n",
    "        \n",
    "#         print(\"\\nPerformance Metrics (Jan 2020 - Dec 2024):\")\n",
    "#         print(\"=\"*50)\n",
    "        \n",
    "#         cw_returns = test_results['CW_Value'].pct_change().dropna()\n",
    "#         ew_returns = test_results['EW_Value'].pct_change().dropna()\n",
    "#         cvar_returns = test_results['CVaR_Value'].pct_change().dropna()\n",
    "#         enhanced_returns = test_results['Enhanced_CVaR_Value'].pct_change().dropna()\n",
    "        \n",
    "#         print(\"\\nCap Weighted Strategy:\")\n",
    "#         cw_metrics = calculate_performance_metrics(cw_returns)\n",
    "#         for k, v in cw_metrics.items():\n",
    "#             print(f\"{k:>20}: {v:.4f}\")\n",
    "        \n",
    "#         print(\"\\nEqual Weighted Strategy:\")\n",
    "#         ew_metrics = calculate_performance_metrics(ew_returns)\n",
    "#         for k, v in ew_metrics.items():\n",
    "#             print(f\"{k:>20}: {v:.4f}\")\n",
    "        \n",
    "#         print(\"\\nStandard CVaR Strategy:\")\n",
    "#         cvar_metrics = calculate_performance_metrics(cvar_returns)\n",
    "#         for k, v in cvar_metrics.items():\n",
    "#             print(f\"{k:>20}: {v:.4f}\")\n",
    "        \n",
    "#         print(\"\\nEnhanced CVaR Strategy:\")\n",
    "#         enhanced_metrics = calculate_performance_metrics(enhanced_returns)\n",
    "#         for k, v in enhanced_metrics.items():\n",
    "#             print(f\"{k:>20}: {v:.4f}\")\n",
    "        \n",
    "#         # 7. Save regime model analysis\n",
    "#         print(\"\\nRegime Model Feature Importance:\")\n",
    "#         print(regime_model.feature_importance)\n",
    "        \n",
    "#         with open('/Volumes/workspace/mixture/etoro_pi/taskC_performance_metrics.txt', 'w') as f:\n",
    "#             f.write(\"Cap Weighted Strategy:\\n\")\n",
    "#             for k, v in cw_metrics.items():\n",
    "#                 f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "            \n",
    "#             f.write(\"\\nEqual Weighted Strategy:\\n\")\n",
    "#             for k, v in ew_metrics.items():\n",
    "#                 f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "            \n",
    "#             f.write(\"\\nStandard CVaR Strategy:\\n\")\n",
    "#             for k, v in cvar_metrics.items():\n",
    "#                 f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "            \n",
    "#             f.write(\"\\nEnhanced CVaR Strategy:\\n\")\n",
    "#             for k, v in enhanced_metrics.items():\n",
    "#                 f.write(f\"{k:>20}: {v:.4f}\\n\")\n",
    "            \n",
    "#             f.write(\"\\nRegime Model Feature Importance:\\n\")\n",
    "#             f.write(regime_model.feature_importance.to_string())\n",
    "        \n",
    "#         print(\"\\nAnalysis complete. Files saved:\")\n",
    "#         print(\"- taskB_enhanced_portfolio_results.csv\")\n",
    "#         print(\"- taskB_enhanced_portfolio_comparison.png\")\n",
    "#         print(\"- taskB_enhanced_performance_metrics.txt\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Fatal error in main execution: {str(e)}\")\n",
    "#         raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Etoro_KM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
